{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add, Lambda, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from spektral.utils import normalized_adjacency\n",
    "from spektral.layers import GCNConv\n",
    "import unicodedata\n",
    "\n",
    "#from scipy.sparse import dok_matrix\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-sperm",
   "metadata": {},
   "source": [
    "# Data load/generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "client = MongoClient()\n",
    "music = client['music_recommender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "turned-mount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6efbfc885a437caa5ccbae88a1d3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a1cd9b1d114e38906957b180d343fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1685524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_social(file_users, file_edges, users_ids):\n",
    "    df_users = pd.read_csv(file_users, sep='\\t', names=['id', 'user'])\n",
    "    df_edges = pd.read_csv(file_edges, sep=' ', names=['origin', 'destination'])\n",
    "    old_new = {}\n",
    "    for _, r in tqdm(df_users.iterrows(), total=len(df_users)):\n",
    "        if r['user'] in users_ids:\n",
    "            old_new[r['id']] = users_ids[r['user']]\n",
    "    social_graph = nx.DiGraph()\n",
    "    social_graph.add_nodes_from(old_new.values())\n",
    "    for _, r in tqdm(df_edges.iterrows(), total=len(df_edges)):\n",
    "        if r['origin'] in old_new and r['destination'] in old_new:\n",
    "            social_graph.add_edge(old_new[r['origin']], old_new[r['destination']])\n",
    "    return social_graph\n",
    "\n",
    "social_graph = load_social('lastfm_sn/lastfm.nodes', 'lastfm_sn/lastfm.edges', dataset['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deluxe-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npos = nx.spring_layout(social_graph, seed=42)\\n\\nnodes = nx.draw_networkx_nodes(social_graph, pos, node_color=\"indigo\")\\nedges = nx.draw_networkx_edges(\\n    social_graph,\\n    pos,\\n    arrowstyle=\"->\",\\n    arrowsize=10,\\n    width=2,\\n)\\n\\npc = mpl.collections.PatchCollection(edges)#, cmap=cmap)\\n#pc.set_array(edge_colors)\\n#plt.colorbar(pc)\\n\\nax = plt.gca()\\nax.set_axis_off()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pos = nx.spring_layout(social_graph, seed=42)\n",
    "\n",
    "nodes = nx.draw_networkx_nodes(social_graph, pos, node_color=\"indigo\")\n",
    "edges = nx.draw_networkx_edges(\n",
    "    social_graph,\n",
    "    pos,\n",
    "    arrowstyle=\"->\",\n",
    "    arrowsize=10,\n",
    "    width=2,\n",
    ")\n",
    "\n",
    "pc = mpl.collections.PatchCollection(edges)#, cmap=cmap)\n",
    "#pc.set_array(edge_colors)\n",
    "#plt.colorbar(pc)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "forward-chambers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = set(social_graph.nodes)\n",
    "ds = nx.to_undirected(social_graph)\n",
    "to_visit = [nodes.pop()]\n",
    "communities = []\n",
    "com = []\n",
    "while len(to_visit) > 0:\n",
    "    c = to_visit.pop()\n",
    "    com.append(c)\n",
    "    next_v = {x for x in ds[c] if x in nodes}\n",
    "    nodes = nodes - next_v\n",
    "    to_visit.extend(next_v)\n",
    "    if len(to_visit) == 0:\n",
    "        communities.append(com)\n",
    "        com = []\n",
    "        if len(nodes) > 0:\n",
    "            to_visit.append(nodes.pop())\n",
    "\n",
    "len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confirmed-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3307.000000</td>\n",
       "      <td>3307.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1653.000000</td>\n",
       "      <td>43.217115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>954.792997</td>\n",
       "      <td>74.060990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>826.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1653.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2479.500000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3306.000000</td>\n",
       "      <td>616.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              node       degree\n",
       "count  3307.000000  3307.000000\n",
       "mean   1653.000000    43.217115\n",
       "std     954.792997    74.060990\n",
       "min       0.000000     0.000000\n",
       "25%     826.500000     4.000000\n",
       "50%    1653.000000    15.000000\n",
       "75%    2479.500000    47.000000\n",
       "max    3306.000000   616.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for n, nbrdict in social_graph.adjacency():\n",
    "    data.append([n, len(nbrdict)])\n",
    "    \n",
    "df = pd.DataFrame(data=data, columns=['node', 'degree'])\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-manner",
   "metadata": {},
   "source": [
    "### Spotify \n",
    "'danceability' : 0\n",
    "\n",
    "'energy' : 1\n",
    "\n",
    "'loudness' : 2 //dividir -60\n",
    "\n",
    "'mode' : 3\n",
    "\n",
    "'speechiness' : 4\n",
    "\n",
    "'acousticness' : 5\n",
    "\n",
    "'instrumentalness' : 6\n",
    "\n",
    "'liveness' : 7\n",
    "\n",
    "'valence' : 8\n",
    "\n",
    "'tempo' : 9 // Dividir 144\n",
    "\n",
    "'key' : Extra\n",
    "\n",
    "Ignora:\n",
    "'duration_ms' : 258787,\n",
    "\n",
    "'time_signature' : 4, 4 el 99% de las veces\n",
    "\n",
    "'spotify_id' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "authentic-weekend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndef spotify_feature_to_np(array, track_id, db_doc):\\n    array[track_id, 0] = db_doc['danceability']\\n    array[track_id, 1] = db_doc['energy']\\n    array[track_id, 2] = db_doc['loudness'] \\n    array[track_id, 3] = db_doc['mode']\\n    array[track_id, 4] = db_doc['speechiness']\\n    array[track_id, 5] = db_doc['acousticness']\\n    array[track_id, 6] = db_doc['instrumentalness']\\n    array[track_id, 7] = db_doc['liveness']\\n    array[track_id, 8] = db_doc['valence']\\n    array[track_id, 9] = db_doc['tempo']\\n    pass\\n\\n\\ndef compute_distance(graph, users, cos):\\n    sim = []\\n    for u in range(users):\\n        for t in graph.neighbors(u):\\n            sim.append(cos[u, t - users])\\n    sim = np.asarray(sim)\\n    mean = np.mean(sim)\\n    std = np.std(sim)\\n    return np.clip(1 - (cos - (mean - 2 * std)) / (4 * std), 0.1, 0.9)\\n    \\n    \\nclass DataGenerator:\\n    \\n    def __init__(self, db, users_tracks, users, tracks, social, cos):\\n        self.users = users\\n        self.tracks = tracks \\n        self.social = social\\n        self.distance = compute_distance(users_tracks, len(users), cos)\\n        self.users_count = len(users)\\n        self.track_count = len(users_tracks.nodes()) - len(users)\\n        self.process_tags_fast(db)\\n        self.process_spotify_fast(db)\\n        self.process_hat_d()\\n        pass   \\n    \\n    def map_id_tracks(self, artist_tracks):\\n        id_track = {}\\n        for a, tracks in artist_tracks.items():\\n            for t, idx in tracks.items():\\n                id_track[idx - self.users_count] = (a, t)\\n        l = [None] * len(id_track)\\n        for i, v in id_track.items():\\n            l[i] = v\\n        return l\\n    \\n    def process_tags_fast(self, db):\\n        print('Processing artists and tags...')\\n        id_track = self.map_id_tracks(self.tracks)\\n        track_id = {t: e for e, t in enumerate(id_track)}\\n        #Set of tags\\n        self.tag_id = {}\\n        self.tracks_tags = [None] * len(id_track)\\n        #Set of artist\\n        self.artist_id = {}\\n        self.tracks_artist = np.zeros((len(id_track), 1), dtype=np.int32) \\n        #Load tag - tracks\\n        save = True\\n        if os.path.exists('data/tags_artist.pickle'):\\n            with open('data/tags_artist.pickle', 'rb') as f:\\n                data = pickle.load(f)\\n            self.tag_id = data['tag_id']\\n            self.artist_id = data['artist_id']\\n            save = False\\n        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \\n                                            {'artist': True, 'track': True, 'tags': True}), \\n                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\\n            if (r['artist'], r['track']) not in track_id:\\n                continue\\n            #Process tags\\n            tags = r['tags']\\n            t_ids = []\\n            for t in tags:\\n                if t not in self.tag_id:\\n                    if not save:\\n                        print('Tag not found: {}'.format(t))\\n                        continue\\n                    self.tag_id[t] = len(self.tag_id) + 1\\n                t_ids.append(self.tag_id[t])\\n            self.tracks_tags[track_id[(r['artist'], r['track'])]] = t_ids\\n            #Process Artist\\n            artist = r['artist']\\n            if artist not in self.artist_id:\\n                if not save:\\n                    print('Artist not found: {}'.format(artist))\\n                    continue\\n                self.artist_id[artist] = len(self.artist_id)\\n            self.tracks_artist[track_id[(r['artist'], r['track'])]] = self.artist_id[artist]\\n        if save:\\n            with open('data/tags_artist.pickle', 'wb') as f:\\n                pickle.dump({'tag_id': self.tag_id, 'artist_id': self.artist_id}, f)\\n        pass\\n    \\n    def process_spotify_fast(self, db):\\n        print('Processing spotify...')\\n        id_track = self.map_id_tracks(self.tracks)\\n        track_id = {t: e for e, t in enumerate(id_track)}\\n        #Load spotify to id track.\\n        spotify_track = {}\\n        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \\n                                            {'artist': True, 'track': True, 'spotify_id': True}), \\n                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\\n            if (r['artist'], r['track']) in track_id:\\n                spotify_track[r['spotify_id']] = (r['artist'], r['track'])\\n        #Load info into a numpy matrix\\n        self.track_spotify_features = np.zeros((len(track_id), 10))\\n        self.track_spotify_key = np.zeros((len(track_id), 1), dtype=np.int8)\\n        for spotify_features in tqdm(music.track_spotify_features.find({}),\\n                                     total=music.track_spotify_features.count_documents({})):\\n            if spotify_features['spotify_id'] not in spotify_track:\\n                continue\\n            t_id = track_id[spotify_track[spotify_features['spotify_id']]]\\n            spotify_feature_to_np(self.track_spotify_features, t_id, spotify_features)\\n            self.track_spotify_key[t_id, 0] = spotify_features['key'] \\n        #Loudness de -60 a 4... Map clip(-60, -2e-4) log\\n        self.track_spotify_features[:, 2] = np.log10(-np.clip(self.track_spotify_features[:, 2], -60, -2e-4))\\n        #Tempo clipped -2, 2 -mean / stdb\\n        tempo_mean = np.mean(self.track_spotify_features[:, 9])\\n        tempo_stdev = np.std(self.track_spotify_features[:, 9])\\n        self.track_spotify_features[:, 9] = np.clip((self.track_spotify_features[:, 9] - tempo_mean) / tempo_stdev, -2, 2)\\n        pass\\n    \\n    def process_hat_d(self):\\n        print('Processing hat d')\\n        a = np.eye(len(self.social.nodes))\\n        for u in tqdm(self.social.nodes):\\n            for n in self.social.neighbors(u):\\n                a[u, n] = 1\\n        self.d_hat = normalized_adjacency(a)\\n        pass\\n    \\n    def get_users_data_slow(self, ids):\\n        users = 0\\n        for u in ids:\\n            neighbors = len(list(self.social.neighbors(u))) \\n            if users < neighbors:\\n                users = neighbors\\n        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\\n        users_graph = np.zeros((ids.shape[0], 1, users + 1))\\n        for i, u in enumerate(ids):\\n            nodes = list(self.social.neighbors(u))\\n            neighbors = np.asarray(nodes)\\n            users_ids[i, 0] = u\\n            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\\n            #Sub Adjacency matrix for the first level neighbors\\n            #this is done because we need the normalize adyacency\\n            id_map = {u: e for e, u in enumerate(nodes, start=1)}\\n            id_map[u] = 0\\n            for u in list(id_map.keys()):\\n                for n in self.social.neighbors(u):\\n                    if n not in id_map:\\n                        id_map[n] = len(id_map)\\n            a = np.eye(len(id_map))\\n            for ui in [u] + list(self.social.neighbors(u)):\\n                for n in self.social.neighbors(ui):\\n                    a[id_map[ui], id_map[n]] = 1\\n            d = normalized_adjacency(a)\\n            users_graph[i, 0, :min(d.shape[1], users + 1)] = d[0, :min(d.shape[1], users + 1)]\\n        return [users_ids, users_graph]\\n    \\n    def get_users_data(self, ids):\\n        users = 0\\n        for u in ids:\\n            neighbors = len(list(self.social.neighbors(u))) \\n            if users < neighbors:\\n                users = neighbors\\n        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\\n        users_graph = np.zeros((ids.shape[0], 1, users + 1))\\n        for i, u in enumerate(ids):\\n            nodes = list(self.social.neighbors(u))\\n            neighbors = np.asarray(nodes)\\n            users_ids[i, 0] = u\\n            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\\n            users_graph[i, 0, 0] = self.d_hat[u, u]\\n            users_graph[i, 0, 1:len(nodes) + 1] = self.d_hat[u, nodes]\\n        return [users_ids, users_graph]\\n        \\n    def get_tracks_data(self, ids):\\n        tracks_ids = ids[:, np.newaxis]\\n        #Tags\\n        tags = []\\n        for i in ids:\\n            tags.append(self.tracks_tags[i])\\n        tags_np = np.zeros((ids.shape[0], max([len(t) for t in tags])), dtype=np.int32)\\n        for i, t in enumerate(tags):\\n            tags_np[i, :len(t)] = t\\n        #Artist\\n        artists = self.tracks_artist[ids, :]\\n        return [tracks_ids, tags_np, artists]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''    \n",
    "def spotify_feature_to_np(array, track_id, db_doc):\n",
    "    array[track_id, 0] = db_doc['danceability']\n",
    "    array[track_id, 1] = db_doc['energy']\n",
    "    array[track_id, 2] = db_doc['loudness'] \n",
    "    array[track_id, 3] = db_doc['mode']\n",
    "    array[track_id, 4] = db_doc['speechiness']\n",
    "    array[track_id, 5] = db_doc['acousticness']\n",
    "    array[track_id, 6] = db_doc['instrumentalness']\n",
    "    array[track_id, 7] = db_doc['liveness']\n",
    "    array[track_id, 8] = db_doc['valence']\n",
    "    array[track_id, 9] = db_doc['tempo']\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_distance(graph, users, cos):\n",
    "    sim = []\n",
    "    for u in range(users):\n",
    "        for t in graph.neighbors(u):\n",
    "            sim.append(cos[u, t - users])\n",
    "    sim = np.asarray(sim)\n",
    "    mean = np.mean(sim)\n",
    "    std = np.std(sim)\n",
    "    return np.clip(1 - (cos - (mean - 2 * std)) / (4 * std), 0.1, 0.9)\n",
    "    \n",
    "    \n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, db, users_tracks, users, tracks, social, cos):\n",
    "        self.users = users\n",
    "        self.tracks = tracks \n",
    "        self.social = social\n",
    "        self.distance = compute_distance(users_tracks, len(users), cos)\n",
    "        self.users_count = len(users)\n",
    "        self.track_count = len(users_tracks.nodes()) - len(users)\n",
    "        self.process_tags_fast(db)\n",
    "        self.process_spotify_fast(db)\n",
    "        self.process_hat_d()\n",
    "        pass   \n",
    "    \n",
    "    def map_id_tracks(self, artist_tracks):\n",
    "        id_track = {}\n",
    "        for a, tracks in artist_tracks.items():\n",
    "            for t, idx in tracks.items():\n",
    "                id_track[idx - self.users_count] = (a, t)\n",
    "        l = [None] * len(id_track)\n",
    "        for i, v in id_track.items():\n",
    "            l[i] = v\n",
    "        return l\n",
    "    \n",
    "    def process_tags_fast(self, db):\n",
    "        print('Processing artists and tags...')\n",
    "        id_track = self.map_id_tracks(self.tracks)\n",
    "        track_id = {t: e for e, t in enumerate(id_track)}\n",
    "        #Set of tags\n",
    "        self.tag_id = {}\n",
    "        self.tracks_tags = [None] * len(id_track)\n",
    "        #Set of artist\n",
    "        self.artist_id = {}\n",
    "        self.tracks_artist = np.zeros((len(id_track), 1), dtype=np.int32) \n",
    "        #Load tag - tracks\n",
    "        save = True\n",
    "        if os.path.exists('data/tags_artist.pickle'):\n",
    "            with open('data/tags_artist.pickle', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            self.tag_id = data['tag_id']\n",
    "            self.artist_id = data['artist_id']\n",
    "            save = False\n",
    "        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \n",
    "                                            {'artist': True, 'track': True, 'tags': True}), \n",
    "                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\n",
    "            if (r['artist'], r['track']) not in track_id:\n",
    "                continue\n",
    "            #Process tags\n",
    "            tags = r['tags']\n",
    "            t_ids = []\n",
    "            for t in tags:\n",
    "                if t not in self.tag_id:\n",
    "                    if not save:\n",
    "                        print('Tag not found: {}'.format(t))\n",
    "                        continue\n",
    "                    self.tag_id[t] = len(self.tag_id) + 1\n",
    "                t_ids.append(self.tag_id[t])\n",
    "            self.tracks_tags[track_id[(r['artist'], r['track'])]] = t_ids\n",
    "            #Process Artist\n",
    "            artist = r['artist']\n",
    "            if artist not in self.artist_id:\n",
    "                if not save:\n",
    "                    print('Artist not found: {}'.format(artist))\n",
    "                    continue\n",
    "                self.artist_id[artist] = len(self.artist_id)\n",
    "            self.tracks_artist[track_id[(r['artist'], r['track'])]] = self.artist_id[artist]\n",
    "        if save:\n",
    "            with open('data/tags_artist.pickle', 'wb') as f:\n",
    "                pickle.dump({'tag_id': self.tag_id, 'artist_id': self.artist_id}, f)\n",
    "        pass\n",
    "    \n",
    "    def process_spotify_fast(self, db):\n",
    "        print('Processing spotify...')\n",
    "        id_track = self.map_id_tracks(self.tracks)\n",
    "        track_id = {t: e for e, t in enumerate(id_track)}\n",
    "        #Load spotify to id track.\n",
    "        spotify_track = {}\n",
    "        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \n",
    "                                            {'artist': True, 'track': True, 'spotify_id': True}), \n",
    "                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\n",
    "            if (r['artist'], r['track']) in track_id:\n",
    "                spotify_track[r['spotify_id']] = (r['artist'], r['track'])\n",
    "        #Load info into a numpy matrix\n",
    "        self.track_spotify_features = np.zeros((len(track_id), 10))\n",
    "        self.track_spotify_key = np.zeros((len(track_id), 1), dtype=np.int8)\n",
    "        for spotify_features in tqdm(music.track_spotify_features.find({}),\n",
    "                                     total=music.track_spotify_features.count_documents({})):\n",
    "            if spotify_features['spotify_id'] not in spotify_track:\n",
    "                continue\n",
    "            t_id = track_id[spotify_track[spotify_features['spotify_id']]]\n",
    "            spotify_feature_to_np(self.track_spotify_features, t_id, spotify_features)\n",
    "            self.track_spotify_key[t_id, 0] = spotify_features['key'] \n",
    "        #Loudness de -60 a 4... Map clip(-60, -2e-4) log\n",
    "        self.track_spotify_features[:, 2] = np.log10(-np.clip(self.track_spotify_features[:, 2], -60, -2e-4))\n",
    "        #Tempo clipped -2, 2 -mean / stdb\n",
    "        tempo_mean = np.mean(self.track_spotify_features[:, 9])\n",
    "        tempo_stdev = np.std(self.track_spotify_features[:, 9])\n",
    "        self.track_spotify_features[:, 9] = np.clip((self.track_spotify_features[:, 9] - tempo_mean) / tempo_stdev, -2, 2)\n",
    "        pass\n",
    "    \n",
    "    def process_hat_d(self):\n",
    "        print('Processing hat d')\n",
    "        a = np.eye(len(self.social.nodes))\n",
    "        for u in tqdm(self.social.nodes):\n",
    "            for n in self.social.neighbors(u):\n",
    "                a[u, n] = 1\n",
    "        self.d_hat = normalized_adjacency(a)\n",
    "        pass\n",
    "    \n",
    "    def get_users_data_slow(self, ids):\n",
    "        users = 0\n",
    "        for u in ids:\n",
    "            neighbors = len(list(self.social.neighbors(u))) \n",
    "            if users < neighbors:\n",
    "                users = neighbors\n",
    "        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\n",
    "        users_graph = np.zeros((ids.shape[0], 1, users + 1))\n",
    "        for i, u in enumerate(ids):\n",
    "            nodes = list(self.social.neighbors(u))\n",
    "            neighbors = np.asarray(nodes)\n",
    "            users_ids[i, 0] = u\n",
    "            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\n",
    "            #Sub Adjacency matrix for the first level neighbors\n",
    "            #this is done because we need the normalize adyacency\n",
    "            id_map = {u: e for e, u in enumerate(nodes, start=1)}\n",
    "            id_map[u] = 0\n",
    "            for u in list(id_map.keys()):\n",
    "                for n in self.social.neighbors(u):\n",
    "                    if n not in id_map:\n",
    "                        id_map[n] = len(id_map)\n",
    "            a = np.eye(len(id_map))\n",
    "            for ui in [u] + list(self.social.neighbors(u)):\n",
    "                for n in self.social.neighbors(ui):\n",
    "                    a[id_map[ui], id_map[n]] = 1\n",
    "            d = normalized_adjacency(a)\n",
    "            users_graph[i, 0, :min(d.shape[1], users + 1)] = d[0, :min(d.shape[1], users + 1)]\n",
    "        return [users_ids, users_graph]\n",
    "    \n",
    "    def get_users_data(self, ids):\n",
    "        users = 0\n",
    "        for u in ids:\n",
    "            neighbors = len(list(self.social.neighbors(u))) \n",
    "            if users < neighbors:\n",
    "                users = neighbors\n",
    "        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\n",
    "        users_graph = np.zeros((ids.shape[0], 1, users + 1))\n",
    "        for i, u in enumerate(ids):\n",
    "            nodes = list(self.social.neighbors(u))\n",
    "            neighbors = np.asarray(nodes)\n",
    "            users_ids[i, 0] = u\n",
    "            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\n",
    "            users_graph[i, 0, 0] = self.d_hat[u, u]\n",
    "            users_graph[i, 0, 1:len(nodes) + 1] = self.d_hat[u, nodes]\n",
    "        return [users_ids, users_graph]\n",
    "        \n",
    "    def get_tracks_data(self, ids):\n",
    "        tracks_ids = ids[:, np.newaxis]\n",
    "        #Tags\n",
    "        tags = []\n",
    "        for i in ids:\n",
    "            tags.append(self.tracks_tags[i])\n",
    "        tags_np = np.zeros((ids.shape[0], max([len(t) for t in tags])), dtype=np.int32)\n",
    "        for i, t in enumerate(tags):\n",
    "            tags_np[i, :len(t)] = t\n",
    "        #Artist\n",
    "        artists = self.tracks_artist[ids, :]\n",
    "        return [tracks_ids, tags_np, artists]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painful-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotify_feature_to_np(array, track_id, db_doc):\n",
    "    array[track_id, 0] = db_doc['danceability']\n",
    "    array[track_id, 1] = db_doc['energy']\n",
    "    array[track_id, 2] = db_doc['loudness'] \n",
    "    array[track_id, 3] = db_doc['mode']\n",
    "    array[track_id, 4] = db_doc['speechiness']\n",
    "    array[track_id, 5] = db_doc['acousticness']\n",
    "    array[track_id, 6] = db_doc['instrumentalness']\n",
    "    array[track_id, 7] = db_doc['liveness']\n",
    "    array[track_id, 8] = db_doc['valence']\n",
    "    array[track_id, 9] = db_doc['tempo']\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_distance(graph, users, cos):\n",
    "    sim = []\n",
    "    for u in range(users):\n",
    "        for t in graph.neighbors(u):\n",
    "            sim.append(cos[u, t - users])\n",
    "    sim = np.asarray(sim)\n",
    "    mean = np.mean(sim)\n",
    "    std = np.std(sim)\n",
    "    return np.clip(1 - (cos - (mean - 2 * std)) / (4 * std), 0.1, 0.9)\n",
    "    \n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nkfd_form = unicodedata.normalize('NFKD', input_str.lower())\n",
    "    return u\"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, db, users_tracks, users, tracks, social, cos):\n",
    "        self.users = users\n",
    "        self.tracks = tracks \n",
    "        self.social = social\n",
    "        self.distance = compute_distance(users_tracks, len(users), cos)\n",
    "        self.users_count = len(users)\n",
    "        self.track_count = len(users_tracks.nodes()) - len(users)\n",
    "        self.process_tags_fast(db)\n",
    "        self.process_spotify_fast(db)\n",
    "        self.process_hat_d()\n",
    "        pass   \n",
    "    \n",
    "    def map_id_tracks(self, artist_tracks):\n",
    "        id_track = defaultdict(list)\n",
    "        for a, tracks in artist_tracks.items():\n",
    "            for t, idx in tracks.items():\n",
    "                id_track[idx - self.users_count].append((a, t))\n",
    "        l = [None] * len(id_track)\n",
    "        for i, v in id_track.items():\n",
    "            l[i] = v\n",
    "        return l\n",
    "    \n",
    "    def process_tags_fast(self, db):\n",
    "        print('Processing artists and tags...')\n",
    "        id_track = self.map_id_tracks(self.tracks)\n",
    "        track_id = {}\n",
    "        for e, tracks in enumerate(id_track):\n",
    "            for t in tracks:\n",
    "                track_id[t] = e\n",
    "        #Set of tags\n",
    "        tracks_tags = [set() for _ in range(len(id_track))] \n",
    "        #Set of artist\n",
    "        tracks_artist = [set() for _ in range(len(id_track))] \n",
    "        #Load tag - tracks\n",
    "        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \n",
    "                                            {'artist': True, 'track': True, 'tags': True}), \n",
    "                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\n",
    "            if (r['artist'], r['track']) not in track_id:\n",
    "                continue\n",
    "            #Process tags\n",
    "            tracks_tags[track_id[(r['artist'], r['track'])]].update([remove_accents(t) for t in r['tags']])\n",
    "            #Process Artist\n",
    "            tracks_artist[track_id[(r['artist'], r['track'])]].add(remove_accents(r['artist']))\n",
    "        #Keep shortest artist\n",
    "        for i in range(len(tracks_artist)):\n",
    "            artists = list(tracks_artist[i])\n",
    "            artists.sort(key=lambda x: (len(x), x))\n",
    "            tracks_artist[i] = artists[0]\n",
    "        #To ids\n",
    "        if os.path.exists('data/tags_artist.pickle'):\n",
    "            with open('data/tags_artist.pickle', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            self.tag_id = data['tag_id']\n",
    "            self.artist_id = data['artist_id']\n",
    "        else:\n",
    "            self.artist_id = {a: e for e, a in enumerate(set(tracks_artist))}\n",
    "            tags_counter = Counter()\n",
    "            for tags in tracks_tags:\n",
    "                for tag in tags:\n",
    "                    tags_counter[tag] += 1\n",
    "            self.tag_id = {}\n",
    "            for tag, c in tags_counter.items():\n",
    "                if c > 4:\n",
    "                    self.tag_id[tag] = len(self.tag_id) + 1\n",
    "            with open('data/tags_artist.pickle', 'wb') as f:\n",
    "                pickle.dump({'tag_id': self.tag_id, 'artist_id': self.artist_id}, f)\n",
    "        self.tracks_tags = [None] * len(tracks_tags)\n",
    "        for i, tags in enumerate(tracks_tags):\n",
    "            ids = [self.tag_id[t] for t in tags if t in self.tag_id]\n",
    "            self.tracks_tags[i] = ids\n",
    "        self.tracks_artist = np.zeros((len(id_track), 1), dtype=np.int32) \n",
    "        for e, a in enumerate(tracks_artist):\n",
    "            self.tracks_artist[e, 0] = self.artist_id[a]\n",
    "        pass\n",
    "    \n",
    "    def process_spotify_fast(self, db):\n",
    "        print('Processing spotify...')\n",
    "        id_track = self.map_id_tracks(self.tracks)\n",
    "        track_id = {}\n",
    "        for e, tracks in enumerate(id_track):\n",
    "            for t in tracks:\n",
    "                track_id[t] = e\n",
    "        #Load spotify to id track.\n",
    "        spotify_track = {}\n",
    "        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \n",
    "                                            {'artist': True, 'track': True, 'spotify_id': True}), \n",
    "                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\n",
    "            if (r['artist'], r['track']) in track_id and r['spotify_id'] not in spotify_track:\n",
    "                spotify_track[r['spotify_id']] = track_id[(r['artist'], r['track'])]\n",
    "        #Load info into a numpy matrix\n",
    "        self.track_spotify_features = np.zeros((len(track_id), 10))\n",
    "        self.track_spotify_key = np.zeros((len(track_id), 1), dtype=np.int8)\n",
    "        for spotify_features in tqdm(music.track_spotify_features.find({}),\n",
    "                                     total=music.track_spotify_features.count_documents({})):\n",
    "            if spotify_features['spotify_id'] not in spotify_track:\n",
    "                continue\n",
    "            t_id = spotify_track[spotify_features['spotify_id']]\n",
    "            spotify_feature_to_np(self.track_spotify_features, t_id, spotify_features)\n",
    "            self.track_spotify_key[t_id, 0] = spotify_features['key'] \n",
    "        #Loudness de -60 a 4... Map clip(-60, -2e-4) log\n",
    "        self.track_spotify_features[:, 2] = np.log10(-np.clip(self.track_spotify_features[:, 2], -60, -2e-4))\n",
    "        #Tempo clipped -2, 2 -mean / stdb\n",
    "        tempo_mean = np.mean(self.track_spotify_features[:, 9])\n",
    "        tempo_stdev = np.std(self.track_spotify_features[:, 9])\n",
    "        self.track_spotify_features[:, 9] = np.clip((self.track_spotify_features[:, 9] - tempo_mean) / tempo_stdev, -2, 2)\n",
    "        pass\n",
    "    \n",
    "    def process_hat_d(self):\n",
    "        print('Processing hat d')\n",
    "        a = np.eye(len(self.social.nodes))\n",
    "        for u in tqdm(self.social.nodes):\n",
    "            for n in self.social.neighbors(u):\n",
    "                a[u, n] = 1\n",
    "        self.d_hat = normalized_adjacency(a)\n",
    "        pass\n",
    "    \n",
    "    def get_users_data_slow(self, ids):\n",
    "        users = 0\n",
    "        for u in ids:\n",
    "            neighbors = len(list(self.social.neighbors(u))) \n",
    "            if users < neighbors:\n",
    "                users = neighbors\n",
    "        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\n",
    "        users_graph = np.zeros((ids.shape[0], 1, users + 1))\n",
    "        for i, u in enumerate(ids):\n",
    "            nodes = list(self.social.neighbors(u))\n",
    "            neighbors = np.asarray(nodes)\n",
    "            users_ids[i, 0] = u\n",
    "            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\n",
    "            #Sub Adjacency matrix for the first level neighbors\n",
    "            #this is done because we need the normalize adyacency\n",
    "            id_map = {u: e for e, u in enumerate(nodes, start=1)}\n",
    "            id_map[u] = 0\n",
    "            for u in list(id_map.keys()):\n",
    "                for n in self.social.neighbors(u):\n",
    "                    if n not in id_map:\n",
    "                        id_map[n] = len(id_map)\n",
    "            a = np.eye(len(id_map))\n",
    "            for ui in [u] + list(self.social.neighbors(u)):\n",
    "                for n in self.social.neighbors(ui):\n",
    "                    a[id_map[ui], id_map[n]] = 1\n",
    "            d = normalized_adjacency(a)\n",
    "            users_graph[i, 0, :min(d.shape[1], users + 1)] = d[0, :min(d.shape[1], users + 1)]\n",
    "        return [users_ids, users_graph]\n",
    "    \n",
    "    def get_users_data(self, ids):\n",
    "        users = 0\n",
    "        for u in ids:\n",
    "            neighbors = len(list(self.social.neighbors(u))) \n",
    "            if users < neighbors:\n",
    "                users = neighbors\n",
    "        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\n",
    "        users_graph = np.zeros((ids.shape[0], 1, users + 1))\n",
    "        for i, u in enumerate(ids):\n",
    "            #Drop of 25% chance\n",
    "            nodes = list(self.social.neighbors(u))\n",
    "            #print(nodes)\n",
    "            neighbors = np.asarray(nodes)\n",
    "            users_ids[i, 0] = u\n",
    "            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\n",
    "            users_graph[i, 0, 0] = self.d_hat[u, u]\n",
    "            #Old users_graph[i, 0, 1:len(nodes) + 1] = self.d_hat[u, nodes]\n",
    "            users_graph[i, 0, 1:len(nodes) + 1] = np.where(np.random.rand(len(nodes))  < 0.9, self.d_hat[u, nodes], 0)\n",
    "            #print('******** users_graph')\n",
    "            #print(users_graph[i,0,:])\n",
    "            #print('******** d_hat')\n",
    "            #print(self.d_hat[u, nodes])\n",
    "            #print(f'{u} {nodes}')\n",
    "            #print('********')\n",
    "        return [users_ids, users_graph]\n",
    "        \n",
    "    def get_tracks_data(self, ids):\n",
    "        tracks_ids = ids[:, np.newaxis]\n",
    "        #Tags\n",
    "        tags = []\n",
    "        for i in ids:\n",
    "            tags.append(self.tracks_tags[i])\n",
    "        tags_np = np.zeros((ids.shape[0], max([len(t) for t in tags])), dtype=np.int32)\n",
    "        for i, t in enumerate(tags):\n",
    "            tags_np[i, :len(t)] = t\n",
    "        #Artist\n",
    "        artists = self.tracks_artist[ids, :]\n",
    "        spotify = self.track_spotify_features[ids, ...]\n",
    "        return [tracks_ids, tags_np, artists, spotify]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grateful-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_superior(data, percentage=0.01, dmin=None, dmax=None):\n",
    "    total = data.shape[0]\n",
    "    if dmin is None:\n",
    "        dmin = np.min(data)\n",
    "    if dmax is None:\n",
    "        dmax = np.max(data)\n",
    "    mid = (dmax + dmin) // 2\n",
    "    while dmin != mid and mid != dmax:\n",
    "        p = np.sum(data >= mid) / total\n",
    "        print(f'{p} {dmin} {mid} {dmax}')\n",
    "        if p > percentage:\n",
    "            dmin = mid\n",
    "        elif p < percentage:\n",
    "            dmax = mid\n",
    "        else:\n",
    "            return mid\n",
    "        mid = (dmax + dmin) // 2\n",
    "    pmin = np.sum(data > dmin) / total\n",
    "    pmax = np.sum(data > dmax) / total\n",
    "    if abs(pmin - percentage) < abs(pmax - percentage):\n",
    "        return dmin\n",
    "    return dmax\n",
    "\n",
    "class TrainGenerator(DataGenerator, Sequence):\n",
    "    \n",
    "    def __init__(self, db, users_tracks, users, tracks, social, cos, batch_size=32, neg_sampling=1, discard_top=None):\n",
    "        super().__init__(db, users_tracks, users, tracks, social, cos)\n",
    "        self.discard_top = discard_top\n",
    "        self.users_tracks = users_tracks\n",
    "        self.batch_size = batch_size\n",
    "        self.neg_sampling = neg_sampling + 1\n",
    "        self.process_user_song()\n",
    "        self.shuffle_indexes = list(range(self.iduser_idtrack.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "   \n",
    "    def process_user_song(self):\n",
    "        print('Processing users-song...')\n",
    "        data_size = 0\n",
    "        for u in range(len(self.users)):\n",
    "            data_size += len(list(self.users_tracks.neighbors(u)))\n",
    "        idx = 0\n",
    "        user_track = np.zeros((data_size, 2), dtype=np.int32) \n",
    "        data = np.zeros((data_size, 2))\n",
    "        with tqdm(total=data_size) as pbar:\n",
    "            for u in range(len(self.users)):\n",
    "                for t, atts in self.users_tracks[u].items():\n",
    "                    t_id = t - len(self.users)\n",
    "                    user_track[idx, 0] = u\n",
    "                    user_track[idx, 1] = t_id\n",
    "                    data[idx, 0] = atts['scrobbles']\n",
    "                    data[idx, 1] = self.distance[u, t_id]\n",
    "                    idx += 1\n",
    "                    pbar.update(1)\n",
    "        self.iduser_idtrack = user_track\n",
    "        if self.discard_top is not None:\n",
    "            limit = search_superior(data[:, 0], percentage=self.discard_top)\n",
    "            data[:, 0] = np.clip(data[:, 0], 0, limit)\n",
    "        self.scrobble_cos = data\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.shuffle_indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(self.neg_sampling * len(self.shuffle_indexes) / self.batch_size)\n",
    "    \n",
    "    def get_neg_sampling(self):\n",
    "        users = np.random.randint(0, self.users_count, size=(self.batch_size,))\n",
    "        tracks = np.random.randint(0, self.track_count, size=(self.batch_size,))\n",
    "        users_data = self.get_users_data(users)\n",
    "        tracks_data = self.get_tracks_data(tracks)\n",
    "        y = np.empty((self.batch_size, 2))\n",
    "        y[:, 0] = 0.5\n",
    "        y[:, 1] = 1 - self.distance[users, tracks]\n",
    "        return users_data + tracks_data, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index % self.neg_sampling == 0:\n",
    "            return self.get_neg_sampling()\n",
    "        index = index // self.neg_sampling\n",
    "        indexes = self.shuffle_indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        users_tracks = self.iduser_idtrack[indexes, ...]\n",
    "        users_data = self.get_users_data(users_tracks[:, 0])\n",
    "        tracks_data = self.get_tracks_data(users_tracks[:, 1])\n",
    "        y = self.scrobble_cos[indexes, ...]\n",
    "        return users_data + tracks_data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46eced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosines = np.load('data/cos.npz')['cosines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prescribed-cattle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing artists and tags...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7295123c8d2643738f3dd527a2790454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/361499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spotify...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ba5a2c60a9446f88e6b88f7283d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/361499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bec9fa831a46d582109055702ea5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hat d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d3a5ae81a64488ac0b146dbb395222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing users-song...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c09f0abba5470bb49ec1f820eaec70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2564908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3392651900185114e-06 1.0 102845.0 205689.0\n",
      "5.0684079117067745e-06 1.0 51423.0 102845.0\n",
      "8.18742816506479e-06 1.0 25712.0 51423.0\n",
      "1.871412152014809e-05 1.0 12856.0 25712.0\n",
      "3.898775316697519e-05 1.0 6428.0 12856.0\n",
      "0.00012593044272932985 1.0 3214.0 6428.0\n",
      "0.0006167862551015476 1.0 1607.0 3214.0\n",
      "0.0024164609412891222 1.0 804.0 1607.0\n",
      "0.008821758909091476 1.0 402.0 804.0\n",
      "0.03042721220410245 1.0 201.0 402.0\n",
      "0.014852774446490868 201.0 301.0 402.0\n",
      "0.020697818401283787 201.0 251.0 301.0\n",
      "0.024889001866733623 201.0 226.0 251.0\n",
      "0.027585004998229957 201.0 213.0 226.0\n",
      "0.02896322207268253 201.0 207.0 213.0\n",
      "0.029709057790766765 201.0 204.0 207.0\n",
      "0.030190166664847237 201.0 202.0 204.0\n",
      "0.029947272962616983 202.0 203.0 204.0\n"
     ]
    }
   ],
   "source": [
    "ds = TrainGenerator(music, dataset['train'], dataset['users'], \n",
    "                   dataset['artist-tracks'], social_graph,\n",
    "                    cosines, discard_top=0.03, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "painted-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 617)\n",
      "(512, 1, 617)\n",
      "(512, 1)\n",
      "(512, 16)\n",
      "(512, 1)\n",
      "(512, 10)\n"
     ]
    }
   ],
   "source": [
    "for i in ds[0][0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f012cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35355339, 0.20412415, 0.14433757, 0.08838835, 0.20412415,\n",
       "       0.1767767 , 0.25      , 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.d_hat[464, [943, 856, 538, 485, 1015, 634, 948, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-yorkshire",
   "metadata": {},
   "source": [
    "# Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "talented-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    #recibe indices con forma 1xvaloresx3 (indices + valor)\n",
    "    #trasnforma los indices a valoresx2 y los valores valoresx1\n",
    "    v_true, dist = y_true[:, 0], y_true[:, 1]\n",
    "    dist = dist ** 0.75\n",
    "    v_true = K.expand_dims(v_true, axis=-1)\n",
    "    dist = K.expand_dims(dist, axis=-1)\n",
    "    return K.mean(dist * K.square(y_pred - K.log(2 * v_true) / K.log(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bridal-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "users (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tracks (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tags (InputLayer)               [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "artist (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_users (Embedding)     (None, None, 64)     211648      users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "user_graph (InputLayer)         [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_tracks (Embedding)    (None, 1, 64)        16128896    tracks[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_tags (Embedding)      (None, None, 64)     540096      tags[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "mask_tags (Lambda)              (None, None)         0           tags[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_artist (Embedding)    (None, 1, 64)        1801728     artist[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcn_user_0 (GCNConv)            (None, None, 32)     2080        embedding_users[0][0]            \n",
      "                                                                 user_graph[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "extract_track (Lambda)          (None, 64)           0           embedding_tracks[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "masked_average_tags (Lambda)    (None, 64)           0           embedding_tags[0][0]             \n",
      "                                                                 mask_tags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "extract_artist (Lambda)         (None, 64)           0           embedding_artist[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "spotify (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "extract_emb_user_base (Lambda)  (None, 64)           0           embedding_users[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "extract_gcn_user (Lambda)       (None, 32)           0           gcn_user_0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_embedding_track_tag (None, 202)          0           extract_track[0][0]              \n",
      "                                                                 masked_average_tags[0][0]        \n",
      "                                                                 extract_artist[0][0]             \n",
      "                                                                 spotify[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_gcn_user_music (Con (None, 298)          0           extract_emb_user_base[0][0]      \n",
      "                                                                 extract_gcn_user[0][0]           \n",
      "                                                                 concatenate_embedding_track_tag_a\n",
      "__________________________________________________________________________________________________\n",
      "deep_dense_1 (Dense)            (None, 256)          76544       concatenate_gcn_user_music[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "deep_dense_2 (Dense)            (None, 256)          65792       deep_dense_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatentate_user_track (Concat (None, 256)          0           extract_emb_user_base[0][0]      \n",
      "                                                                 extract_track[0][0]              \n",
      "                                                                 masked_average_tags[0][0]        \n",
      "                                                                 extract_artist[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "deep_dense_3 (Dense)            (None, 1)            257         deep_dense_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "wide (Dense)                    (None, 1)            257         concatentate_user_track[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "deep_plus_wide (Add)            (None, 1)            0           deep_dense_3[0][0]               \n",
      "                                                                 wide[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 18,827,298\n",
      "Trainable params: 18,827,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 64\n",
    "kernel_size = 32\n",
    "deep = 1\n",
    "\n",
    "#Input users\n",
    "i_users = Input((None,), name='users')\n",
    "i_user_graph = Input((None, None), name='user_graph')\n",
    "\n",
    "#Input music\n",
    "i_tracks = Input((1,), name='tracks')\n",
    "i_tags = Input((None,), name='tags')\n",
    "i_artist = Input((1,), name='artist')\n",
    "i_spotify = Input((10,), name='spotify')\n",
    "\n",
    "#Process users\n",
    "emb_user_base = Embedding(len(dataset['users']), emb_size, name=\"embedding_users\")(i_users)\n",
    "emb_user = emb_user_base\n",
    "emb_user_base = Lambda(lambda x: x[:, 0,:], name='extract_emb_user_base')(emb_user_base)\n",
    "for i in range(deep):\n",
    "    emb_user = GCNConv(kernel_size, name='gcn_user_{}'.format(i))([emb_user, i_user_graph])\n",
    "\n",
    "emb_user = Lambda(lambda x: x[:, 0,:], name='extract_gcn_user')(emb_user)\n",
    "#Process music\n",
    "def avg(x):\n",
    "    i = x[0]\n",
    "    m = x[1]\n",
    "    i = i * tf.expand_dims(tf.cast(m, tf.float32), axis=-1)\n",
    "    r = tf.reduce_sum(i, axis=-2) / tf.expand_dims(tf.reduce_sum(tf.cast(m, tf.float32), axis=-1), axis=-1)\n",
    "    return tf.where(tf.math.logical_or(tf.math.is_nan(r), tf.math.is_inf(r)), 0., r)\n",
    "\n",
    "emb_tracks = Embedding(len(dataset['train'].nodes()) - len(dataset['users']), emb_size, name=\"embedding_tracks\")(i_tracks)\n",
    "emb_tracks = Lambda(lambda x: x[:, 0, :], name='extract_track')(emb_tracks)\n",
    "\n",
    "mask_tags = Lambda(lambda x: x != 0, name='mask_tags')(i_tags)\n",
    "emb_tags = Embedding(len(ds.tag_id) + 1, emb_size, name='embedding_tags')(i_tags)\n",
    "emb_tags = Lambda(avg, name='masked_average_tags')([emb_tags, mask_tags])\n",
    "\n",
    "emb_artist = Embedding(len(ds.artist_id), emb_size, name='embedding_artist')(i_artist)\n",
    "emb_artist = Lambda(lambda x: x[:, 0, :], name='extract_artist')(emb_artist)\n",
    "\n",
    "emb_music = Concatenate(name='concatenate_embedding_track_tag_artist')([emb_tracks, emb_tags, emb_artist, i_spotify])\n",
    "#emb_music = Dense(kernel_size, name='dense_music')(emb_music)\n",
    "\n",
    "#Deep part\n",
    "deep = Concatenate(name='concatenate_gcn_user_music')([emb_user_base, emb_user, emb_music])\n",
    "deep = Dense(256, name='deep_dense_1')(deep)\n",
    "deep = Dense(256, name='deep_dense_2')(deep)\n",
    "deep = Dense(1, name='deep_dense_3')(deep)\n",
    "\n",
    "#Wide \n",
    "wide = Concatenate(name='concatentate_user_track')([emb_user_base, emb_tracks, emb_tags, emb_artist])\n",
    "wide = Dense(1, name='wide')(wide)\n",
    "\n",
    "out = Add(name='deep_plus_wide')([deep, wide])\n",
    "\n",
    "model = Model([i_users, i_user_graph, i_tracks, i_tags, i_artist, i_spotify], out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3117ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  464/10020 [>.............................] - ETA: 1:07:18 - loss: 2.0441"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import os\n",
    "if not os.path.exists('model_dropout_complex_075'):\n",
    "    os.makedirs('model_dropout_complex_075')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 5 or epoch % 2 != 0:\n",
    "        return lr\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "model.fit(ds, epochs=10, callbacks=[ModelCheckpoint(filepath='model_dropout_complex_075/{epoch:02d}-model-{loss:.5f}.hdf5',\n",
    "                                                        monitor='loss',\n",
    "                                                        save_best_only=False),\n",
    "                                         LearningRateScheduler(scheduler)], workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c946a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_dropout_complex_075/model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
