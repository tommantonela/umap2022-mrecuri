{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add, Lambda, Concatenate, Dense, BatchNormalization, ELU\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from spektral.utils import normalized_adjacency\n",
    "from spektral.layers import GCNConv\n",
    "#from scipy.sparse import dok_matrix\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-rouge",
   "metadata": {},
   "source": [
    "# Data load/generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "client = MongoClient()\n",
    "music = client['music_recommender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_social(file_users, file_edges, users_ids):\n",
    "    df_users = pd.read_csv(file_users, sep='\\t', names=['id', 'user'])\n",
    "    df_edges = pd.read_csv(file_edges, sep=' ', names=['origin', 'destination'])\n",
    "    old_new = {}\n",
    "    for _, r in tqdm(df_users.iterrows(), total=len(df_users)):\n",
    "        if r['user'] in users_ids:\n",
    "            old_new[r['id']] = users_ids[r['user']]\n",
    "    social_graph = nx.DiGraph()\n",
    "    social_graph.add_nodes_from(old_new.values())\n",
    "    for _, r in tqdm(df_edges.iterrows(), total=len(df_edges)):\n",
    "        if r['origin'] in old_new and r['destination'] in old_new:\n",
    "            social_graph.add_edge(old_new[r['origin']], old_new[r['destination']])\n",
    "    return social_graph\n",
    "\n",
    "social_graph = load_social('lastfm_sn/lastfm.nodes', 'lastfm_sn/lastfm.edges', dataset['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pos = nx.spring_layout(social_graph, seed=42)\n",
    "\n",
    "nodes = nx.draw_networkx_nodes(social_graph, pos, node_color=\"indigo\")\n",
    "edges = nx.draw_networkx_edges(\n",
    "    social_graph,\n",
    "    pos,\n",
    "    arrowstyle=\"->\",\n",
    "    arrowsize=10,\n",
    "    width=2,\n",
    ")\n",
    "\n",
    "pc = mpl.collections.PatchCollection(edges)#, cmap=cmap)\n",
    "#pc.set_array(edge_colors)\n",
    "#plt.colorbar(pc)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(social_graph.nodes)\n",
    "ds = nx.to_undirected(social_graph)\n",
    "to_visit = [nodes.pop()]\n",
    "communities = []\n",
    "com = []\n",
    "while len(to_visit) > 0:\n",
    "    c = to_visit.pop()\n",
    "    com.append(c)\n",
    "    next_v = {x for x in ds[c] if x in nodes}\n",
    "    nodes = nodes - next_v\n",
    "    to_visit.extend(next_v)\n",
    "    if len(to_visit) == 0:\n",
    "        communities.append(com)\n",
    "        com = []\n",
    "        if len(nodes) > 0:\n",
    "            to_visit.append(nodes.pop())\n",
    "\n",
    "len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for n, nbrdict in social_graph.adjacency():\n",
    "    data.append([n, len(nbrdict)])\n",
    "    \n",
    "df = pd.DataFrame(data=data, columns=['node', 'degree'])\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-assessment",
   "metadata": {},
   "source": [
    "### Spotify \n",
    "'danceability' : 0\n",
    "\n",
    "'energy' : 1\n",
    "\n",
    "'loudness' : 2 //dividir -60\n",
    "\n",
    "'mode' : 3\n",
    "\n",
    "'speechiness' : 4\n",
    "\n",
    "'acousticness' : 5\n",
    "\n",
    "'instrumentalness' : 6\n",
    "\n",
    "'liveness' : 7\n",
    "\n",
    "'valence' : 8\n",
    "\n",
    "'tempo' : 9 // Dividir 144\n",
    "\n",
    "'key' : Extra\n",
    "\n",
    "Ignora:\n",
    "'duration_ms' : 258787,\n",
    "\n",
    "'time_signature' : 4, 4 el 99% de las veces\n",
    "\n",
    "'spotify_id' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotify_feature_to_np(array, track_id, db_doc):\n",
    "    array[track_id, 0] = db_doc['danceability']\n",
    "    array[track_id, 1] = db_doc['energy']\n",
    "    array[track_id, 2] = db_doc['loudness'] \n",
    "    array[track_id, 3] = db_doc['mode']\n",
    "    array[track_id, 4] = db_doc['speechiness']\n",
    "    array[track_id, 5] = db_doc['acousticness']\n",
    "    array[track_id, 6] = db_doc['instrumentalness']\n",
    "    array[track_id, 7] = db_doc['liveness']\n",
    "    array[track_id, 8] = db_doc['valence']\n",
    "    array[track_id, 9] = db_doc['tempo']\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_distance(graph, users, cos):\n",
    "    sim = []\n",
    "    for u in range(users):\n",
    "        for t in graph.neighbors(u):\n",
    "            sim.append(cos[u, t - users])\n",
    "    sim = np.asarray(sim)\n",
    "    mean = np.mean(sim)\n",
    "    std = np.std(sim)\n",
    "    return np.clip(1 - (cos - (mean - 2 * std)) / (4 * std), 0.1, 0.9)\n",
    "    \n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nkfd_form = unicodedata.normalize('NFKD', input_str.lower())\n",
    "    return u\"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, db, users_tracks, users, tracks, social, cos):\n",
    "        self.users = users\n",
    "        self.tracks = tracks \n",
    "        self.social = social\n",
    "        self.distance = compute_distance(users_tracks, len(users), cos)\n",
    "        self.users_count = len(users)\n",
    "        self.track_count = len(users_tracks.nodes()) - len(users)\n",
    "        self.process_tags_fast(db)\n",
    "        self.process_spotify_fast(db)\n",
    "        self.process_hat_d()\n",
    "        pass   \n",
    "    \n",
    "    def map_id_tracks(self, artist_tracks):\n",
    "        id_track = defaultdict(list)\n",
    "        for a, tracks in artist_tracks.items():\n",
    "            for t, idx in tracks.items():\n",
    "                id_track[idx - self.users_count].append((a, t))\n",
    "        l = [None] * len(id_track)\n",
    "        for i, v in id_track.items():\n",
    "            l[i] = v\n",
    "        return l\n",
    "    \n",
    "    def process_tags_fast(self, db):\n",
    "        print('Processing artists and tags...')\n",
    "        id_track = self.map_id_tracks(self.tracks)\n",
    "        track_id = {}\n",
    "        for e, tracks in enumerate(id_track):\n",
    "            for t in tracks:\n",
    "                track_id[t] = e\n",
    "        #Set of tags\n",
    "        tracks_tags = [set() for _ in range(len(id_track))] \n",
    "        #Set of artist\n",
    "        tracks_artist = [set() for _ in range(len(id_track))] \n",
    "        #Load tag - tracks\n",
    "        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \n",
    "                                            {'artist': True, 'track': True, 'tags': True}), \n",
    "                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\n",
    "            if (r['artist'], r['track']) not in track_id:\n",
    "                continue\n",
    "            #Process tags\n",
    "            tracks_tags[track_id[(r['artist'], r['track'])]].update([remove_accents(t) for t in r['tags']])\n",
    "            #Process Artist\n",
    "            tracks_artist[track_id[(r['artist'], r['track'])]].add(remove_accents(r['artist']))\n",
    "        #Keep shortest artist\n",
    "        for i in range(len(tracks_artist)):\n",
    "            artists = list(tracks_artist[i])\n",
    "            artists.sort(key=lambda x: (len(x), x))\n",
    "            tracks_artist[i] = artists[0]\n",
    "        #To ids\n",
    "        if os.path.exists('data/tags_artist.pickle'):\n",
    "            with open('data/tags_artist.pickle', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            self.tag_id = data['tag_id']\n",
    "            self.artist_id = data['artist_id']\n",
    "        else:\n",
    "            self.artist_id = {a: e for e, a in enumerate(set(tracks_artist))}\n",
    "            tags_counter = Counter()\n",
    "            for tags in tracks_tags:\n",
    "                for tag in tags:\n",
    "                    tags_counter[tag] += 1\n",
    "            self.tag_id = {}\n",
    "            for tag, c in tags_counter.items():\n",
    "                if c > 4:\n",
    "                    self.tag_id[tag] = len(self.tag_id) + 1\n",
    "            with open('data/tags_artist.pickle', 'wb') as f:\n",
    "                pickle.dump({'tag_id': self.tag_id, 'artist_id': self.artist_id}, f)\n",
    "        self.tracks_tags = [None] * len(tracks_tags)\n",
    "        for i, tags in enumerate(tracks_tags):\n",
    "            ids = [self.tag_id[t] for t in tags if t in self.tag_id]\n",
    "            self.tracks_tags[i] = ids\n",
    "        self.tracks_artist = np.zeros((len(id_track), 1), dtype=np.int32) \n",
    "        for e, a in enumerate(tracks_artist):\n",
    "            self.tracks_artist[e, 0] = self.artist_id[a]\n",
    "        pass\n",
    "    \n",
    "    def process_spotify_fast(self, db):\n",
    "        print('Processing spotify...')\n",
    "        id_track = self.map_id_tracks(self.tracks)\n",
    "        track_id = {}\n",
    "        for e, tracks in enumerate(id_track):\n",
    "            for t in tracks:\n",
    "                track_id[t] = e\n",
    "        #Load spotify to id track.\n",
    "        spotify_track = {}\n",
    "        for r in tqdm(music.track_info.find({'spotify_id': {'$exists': True}}, \n",
    "                                            {'artist': True, 'track': True, 'spotify_id': True}), \n",
    "                      total=music.track_info.count_documents({'spotify_id': {'$exists': True}})):\n",
    "            if (r['artist'], r['track']) in track_id and r['spotify_id'] not in spotify_track:\n",
    "                spotify_track[r['spotify_id']] = track_id[(r['artist'], r['track'])]\n",
    "        #Load info into a numpy matrix\n",
    "        self.track_spotify_features = np.zeros((len(track_id), 10))\n",
    "        self.track_spotify_key = np.zeros((len(track_id), 1), dtype=np.int8)\n",
    "        for spotify_features in tqdm(music.track_spotify_features.find({}),\n",
    "                                     total=music.track_spotify_features.count_documents({})):\n",
    "            if spotify_features['spotify_id'] not in spotify_track:\n",
    "                continue\n",
    "            t_id = spotify_track[spotify_features['spotify_id']]\n",
    "            spotify_feature_to_np(self.track_spotify_features, t_id, spotify_features)\n",
    "            self.track_spotify_key[t_id, 0] = spotify_features['key'] \n",
    "        #Loudness de -60 a 4... Map clip(-60, -2e-4) log\n",
    "        self.track_spotify_features[:, 2] = np.log10(-np.clip(self.track_spotify_features[:, 2], -60, -2e-4))\n",
    "        #Tempo clipped -2, 2 -mean / stdb\n",
    "        tempo_mean = np.mean(self.track_spotify_features[:, 9])\n",
    "        tempo_stdev = np.std(self.track_spotify_features[:, 9])\n",
    "        self.track_spotify_features[:, 9] = np.clip((self.track_spotify_features[:, 9] - tempo_mean) / tempo_stdev, -2, 2)\n",
    "        pass\n",
    "    \n",
    "    def process_hat_d(self):\n",
    "        print('Processing hat d')\n",
    "        a = np.eye(len(self.social.nodes))\n",
    "        for u in tqdm(self.social.nodes):\n",
    "            for n in self.social.neighbors(u):\n",
    "                a[u, n] = 1\n",
    "        self.d_hat = normalized_adjacency(a)\n",
    "        pass\n",
    "    \n",
    "    def get_users_data_slow(self, ids):\n",
    "        users = 0\n",
    "        for u in ids:\n",
    "            neighbors = len(list(self.social.neighbors(u))) \n",
    "            if users < neighbors:\n",
    "                users = neighbors\n",
    "        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\n",
    "        users_graph = np.zeros((ids.shape[0], 1, users + 1))\n",
    "        for i, u in enumerate(ids):\n",
    "            nodes = list(self.social.neighbors(u))\n",
    "            neighbors = np.asarray(nodes)\n",
    "            users_ids[i, 0] = u\n",
    "            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\n",
    "            #Sub Adjacency matrix for the first level neighbors\n",
    "            #this is done because we need the normalize adyacency\n",
    "            id_map = {u: e for e, u in enumerate(nodes, start=1)}\n",
    "            id_map[u] = 0\n",
    "            for u in list(id_map.keys()):\n",
    "                for n in self.social.neighbors(u):\n",
    "                    if n not in id_map:\n",
    "                        id_map[n] = len(id_map)\n",
    "            a = np.eye(len(id_map))\n",
    "            for ui in [u] + list(self.social.neighbors(u)):\n",
    "                for n in self.social.neighbors(ui):\n",
    "                    a[id_map[ui], id_map[n]] = 1\n",
    "            d = normalized_adjacency(a)\n",
    "            users_graph[i, 0, :min(d.shape[1], users + 1)] = d[0, :min(d.shape[1], users + 1)]\n",
    "        return [users_ids, users_graph]\n",
    "    \n",
    "    def get_users_data(self, ids):\n",
    "        users = 0\n",
    "        for u in ids:\n",
    "            neighbors = len(list(self.social.neighbors(u))) \n",
    "            if users < neighbors:\n",
    "                users = neighbors\n",
    "        users_ids = np.zeros((ids.shape[0], users + 1), dtype=np.int32)\n",
    "        users_graph = np.zeros((ids.shape[0], 1, users + 1))\n",
    "        for i, u in enumerate(ids):\n",
    "            nodes = list(self.social.neighbors(u))\n",
    "            neighbors = np.asarray(nodes)\n",
    "            users_ids[i, 0] = u\n",
    "            users_ids[i, 1:neighbors.shape[0] + 1] = neighbors\n",
    "            users_graph[i, 0, 0] = self.d_hat[u, u]\n",
    "            users_graph[i, 0, 1:len(nodes) + 1] = self.d_hat[u, nodes]\n",
    "        return [users_ids, users_graph]\n",
    "        \n",
    "    def get_tracks_data(self, ids):\n",
    "        tracks_ids = ids[:, np.newaxis]\n",
    "        #Tags\n",
    "        tags = []\n",
    "        for i in ids:\n",
    "            tags.append(self.tracks_tags[i])\n",
    "        tags_np = np.zeros((ids.shape[0], max([len(t) for t in tags])), dtype=np.int32)\n",
    "        for i, t in enumerate(tags):\n",
    "            tags_np[i, :len(t)] = t\n",
    "        #Artist\n",
    "        artists = self.tracks_artist[ids, :]\n",
    "        spotify = self.track_spotify_features[ids, ...]\n",
    "        return [tracks_ids, tags_np, artists, spotify]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenUserTrack(DataGenerator, Sequence):\n",
    "    \n",
    "    def __init__(self, db, users_tracks, users, tracks, social, cos, batch_size=512, mode_user=True):\n",
    "        super().__init__(db, users_tracks, users, tracks, social, cos)\n",
    "        self.users_tracks = users_tracks\n",
    "        self.batch_size = batch_size\n",
    "        self.mode_user = mode_user\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.mode_user:\n",
    "            return math.ceil(self.users_count / self.batch_size)\n",
    "        return math.ceil(self.track_count / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode_user:\n",
    "            users = np.arange(index * self.batch_size, min((index + 1) * self.batch_size, self.users_count))\n",
    "            return self.get_users_data(users)\n",
    "        tracks = np.arange(index * self.batch_size, min((index + 1) * self.batch_size, self.track_count))\n",
    "        return self.get_tracks_data(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-teens",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = GenUserTrack(music, dataset['train'], dataset['users'], \n",
    "                  dataset['artist-tracks'], social_graph,\n",
    "                  np.load('data/cos.npz')['cosines'])#, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ds[0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-sperm",
   "metadata": {},
   "source": [
    "# Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    #recibe indices con forma 1xvaloresx3 (indices + valor)\n",
    "    #trasnforma los indices a valoresx2 y los valores valoresx1\n",
    "    v_true, dist = y_true[:, 0], y_true[:, 1]\n",
    "    return K.mean(dist * K.square(y_pred - K.log(2 * v_true) / K.log(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 64\n",
    "kernel_size = 32\n",
    "deep = 1\n",
    "\n",
    "#Input users\n",
    "i_users = Input((None,), name='users')\n",
    "i_user_graph = Input((None, None), name='user_graph')\n",
    "\n",
    "#Input music\n",
    "i_tracks = Input((1,), name='tracks')\n",
    "i_tags = Input((None,), name='tags')\n",
    "i_artist = Input((1,), name='artist')\n",
    "i_spotify = Input((10,), name='spotify')\n",
    "\n",
    "#Process users\n",
    "emb_user_base = Embedding(len(dataset['users']), emb_size, name=\"embedding_users\")(i_users)\n",
    "emb_user = emb_user_base\n",
    "emb_user_base = Lambda(lambda x: x[:, 0,:], name='extract_emb_user_base')(emb_user_base)\n",
    "for i in range(deep):\n",
    "    emb_user = GCNConv(kernel_size, name='gcn_user_{}'.format(i))([emb_user, i_user_graph])\n",
    "\n",
    "emb_user = Lambda(lambda x: x[:, 0,:], name='extract_gcn_user')(emb_user)\n",
    "#Process music\n",
    "def avg(x):\n",
    "    i = x[0]\n",
    "    m = x[1]\n",
    "    i = i * tf.expand_dims(tf.cast(m, tf.float32), axis=-1)\n",
    "    r = tf.reduce_sum(i, axis=-2) / tf.expand_dims(tf.reduce_sum(tf.cast(m, tf.float32), axis=-1), axis=-1)\n",
    "    return tf.where(tf.math.logical_or(tf.math.is_nan(r), tf.math.is_inf(r)), 0., r)\n",
    "\n",
    "emb_tracks = Embedding(len(dataset['train'].nodes()) - len(dataset['users']), emb_size, name=\"embedding_tracks\")(i_tracks)\n",
    "emb_tracks = Lambda(lambda x: x[:, 0, :], name='extract_track')(emb_tracks)\n",
    "\n",
    "mask_tags = Lambda(lambda x: x != 0, name='mask_tags')(i_tags)\n",
    "emb_tags = Embedding(len(ds.tag_id) + 1, emb_size, name='embedding_tags')(i_tags)\n",
    "emb_tags = Lambda(avg, name='masked_average_tags', output_shape=(64,))([emb_tags, mask_tags])\n",
    "\n",
    "emb_artist = Embedding(len(ds.artist_id), emb_size, name='embedding_artist')(i_artist)\n",
    "emb_artist = Lambda(lambda x: x[:, 0, :], name='extract_artist')(emb_artist)\n",
    "\n",
    "emb_music = Concatenate(name='concatenate_embedding_track_tag_artist')([emb_tracks, emb_tags, emb_artist, i_spotify])\n",
    "#emb_music = Dense(kernel_size, name='dense_music')(emb_music)\n",
    "\n",
    "#Deep part\n",
    "deep = Concatenate(name='concatenate_gcn_user_music')([emb_user_base, emb_user, emb_music])\n",
    "deep = Dense(256, name='deep_dense_1')(deep)\n",
    "deep = Dense(256, name='deep_dense_2')(deep)\n",
    "deep = Dense(1, name='deep_dense_3')(deep)\n",
    "\n",
    "#Wide \n",
    "wide = Concatenate(name='concatentate_user_track')([emb_user_base, emb_tracks, emb_tags, emb_artist])\n",
    "wide = Dense(1, name='wide')(wide)\n",
    "\n",
    "out = Add(name='deep_plus_wide')([deep, wide])\n",
    "\n",
    "model = Model([i_users, i_user_graph, i_tracks, i_tags, i_artist, i_spotify], out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_dropout_complex_075/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_user = Model([i_users, i_user_graph], [emb_user, emb_user_base])\n",
    "model_track = Model([i_tracks, i_tags, i_artist, i_spotify], [emb_music, emb_tracks, emb_tags, emb_artist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-attempt",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds.mode_user = True\n",
    "users = model_user.predict(ds)\n",
    "ds.mode_user = False\n",
    "tracks = model_track.predict(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_emb_user = Input(emb_user.shape[1:], name='i_emb_user')\n",
    "i_emb_user_base = Input(emb_user_base.shape[1:], name='i_emb_user_base')\n",
    "i_emb_music = Input(emb_music.shape[1:], name='i_emb_music')\n",
    "i_emb_tracks = Input(emb_tracks.shape[1:], name='i_emb_tracks')\n",
    "i_emb_tags = Input(emb_tags.shape[1:], name='i_emb_tags')\n",
    "i_emb_artist = Input(emb_artist.shape[1:], name='i_emb_artist')\n",
    "#Deep part\n",
    "deep = Concatenate(name='concatenate_gcn_user_music')([i_emb_user_base, i_emb_user, i_emb_music])\n",
    "deep = Dense(256, name='deep_dense_1')(deep)\n",
    "deep = Dense(256, name='deep_dense_2')(deep)\n",
    "deep = Dense(1, name='deep_dense_3')(deep)\n",
    "\n",
    "\n",
    "#Wide \n",
    "wide = Concatenate(name='concatentate_user_track')([i_emb_user_base, i_emb_tracks, i_emb_tags, i_emb_artist])\n",
    "wide = Dense(1, name='wide')(wide)\n",
    "\n",
    "out = Add(name='deep_plus_wide')([deep, wide])\n",
    "\n",
    "model_fast = Model([i_emb_user, i_emb_user_base, i_emb_music, i_emb_tracks, i_emb_tags, i_emb_artist], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l_in in model_fast.layers:\n",
    "    for l_out in model.layers:\n",
    "        if l_in.name == l_out.name:\n",
    "            l_in.set_weights(l_out.get_weights())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ds.get_users_data(np.arange(10)) + ds.get_tracks_data(np.arange(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fast.predict([users[0][:10,...], users[1][:10,...], tracks[0][:10,...], tracks[1][:10,...], tracks[2][:10,...], tracks[3][:10,...]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_users = len(dataset['users'])\n",
    "l_tracks = len(dataset['train'].nodes) - len(dataset['users'])\n",
    "\n",
    "r_users = []\n",
    "r_tracks = []\n",
    "for n in dataset['test'].nodes:\n",
    "    if n < l_users:\n",
    "        r_users.append(n)\n",
    "    else:\n",
    "        r_tracks.append(n)\n",
    "\n",
    "\n",
    "r_users.sort()\n",
    "r_tracks.sort()\n",
    "r_tracks = np.asarray(r_tracks)\n",
    "r_user_id = {u: e for e, u in enumerate(r_users)}\n",
    "r_track_id = {u: e for e, u in enumerate(r_tracks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tracks = [t[r_tracks - l_users, ...] for t in tracks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-forum",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "df = pd.DataFrame(data=[], columns=['User', 'Track', 'Prediction']).astype({'User': np.int32, 'Track': np.int32, 'Prediction': np.float32})\n",
    "\n",
    "parquet_schema = pa.Table.from_pandas(df=df, preserve_index=False).schema\n",
    "# Open a Parquet file for writing\n",
    "parquet_writer = pq.ParquetWriter('model_dropout_complex_075/predictions-dropout_complex_075.parquet', parquet_schema, compression='GZIP')\n",
    "\n",
    "partial = []\n",
    "for u in tqdm(r_users):\n",
    "    user_aux = [np.repeat(users[0][u:u+1, ...], len(r_tracks), axis=0), np.repeat(users[1][u:u+1, ...], len(r_tracks), axis=0)]\n",
    "    pred = model_fast.predict(user_aux + n_tracks, batch_size=len(r_tracks))\n",
    "    data = np.zeros((len(r_tracks), 3), dtype=np.float32)\n",
    "    data[:, 0] = u\n",
    "    data[:, 1] = r_tracks\n",
    "    data[:, 2] = pred[:, 0]\n",
    "    df = pd.DataFrame(data=data, columns=['User', 'Track', 'Prediction']).astype({'User': np.int32, 'Track': np.int32, 'Prediction': np.float32})\n",
    "    \n",
    "    no_valid = set(dataset['train'].neighbors(u))\n",
    "    df = df[~df['Track'].isin(no_valid)]\n",
    "    \n",
    "    partial.append(df)\n",
    "    if len(partial) >= 100:\n",
    "        df = pd.concat(partial)\n",
    "        partial = []\n",
    "        table = pa.Table.from_pandas(df, schema=parquet_schema)\n",
    "        parquet_writer.write_table(table)\n",
    "        \n",
    "        \n",
    "if len(partial) > 0:\n",
    "    df = pd.concat(partial)\n",
    "    partial = []\n",
    "    table = pa.Table.from_pandas(df, schema=parquet_schema)\n",
    "    parquet_writer.write_table(table)\n",
    "    \n",
    "parquet_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-routine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
